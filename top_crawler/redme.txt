实现：
    图片下载
    URL解析（排除不相干图片）
    命令行
    协程
    信号量
    日志


思考：如果下次要爬其他的美女网站，这个程序如何尽可能利于复用?
    
    爬虫一般就分为两步，分析信息，爬取信息。
    每个爬虫的需求不同，并且每个网站的网页布局都不同，在这个方面实现复用，不太容易。
    但是，我想可以手动生成一个 path_dict。
    比如说，这次这个网站的美女图片，距离首页2（深度为2）。
    先在第一个路径（/html/body/div[@id="wrapper"]/div[@id="wrapper-inner"]/div[@id="canvas"]/div[@id="content"]/div[@class="catalog"]）下，找到正确的URL位置；
    再通过第二个路径（div[@class="e m"]/a），获取所有的URL；
    再通过第三个路径（/html/body/div[@id="wrapper"]/div[@id="wrapper-inner"]/div[@id="mainbox"]/div[@id="canvasbox"]/div[@id="content"]/a[@id="item-tip"]），找到图片的URL；
    最后，下载下来。

    在这个过程中，前面的三步，都需要去找到路径，每次爬其他网站的时候，就把路径分析出来，然后，其余的就可以交给程序去做了。
    所以，可以手动生成一个path_dict，key定义级别（级别越高，离目标越远），value就是path。
    每分析完一个path，得到分析结果（url），再将分析结果和下一级path对应。。。。
    最后出来的结果，就是 目标了。
    爬取信息的部分，基本上都是一样的，所以，爬取信息的部分，可以写成一个类，复用。还有，像log之类的，都可复用。

    这个是比较老实的思路，还有一种不老实的方法。
    最后，我们想要的目标，是可以下载到图片的URL。
    URL = 协议 ：// 域名 /文件路径/文件名
    所以，我们想要的目标，就在某几个文件路径之下，几百个url就可以分析出来。
    然后，遍历网站，并且通过正则，提取出整个网站的URL，URL匹配的就留下来，下载，不匹配的，就丢弃～
  

    以上就是我的想法了～